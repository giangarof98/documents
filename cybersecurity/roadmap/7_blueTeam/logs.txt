Logs and Analytics
    Log collection and analysis, in many ways, is the backbone of a SOC analyst’s role. 
    Without knowing how to gather and analyze logs, it is nearly impossible to tell what 
        is happening within a system. 

    There are many different ways/tools in which logs can be collected, 
        and just as many different types of logs can be analyzed.
    
    Some of those may already be in human-readable text, 
        and some must be converted into something we can analyze.

    
    Logs record events occurring within an organization’s system/network. 
    They are composed of log entries; each contains information about a 
        specific event within the system. Logs are generated by many sources, 
        such as security software, antivirus software, firewalls, 
        intrusion detection/protection systems, operating systems on servers, 
        workstations, networking equipment, and applications. 
        Capturing as much contextual data as possible up front, 
        including the version of the system being interacted with and what users 
        of the system can access, is critical to understanding log data and 
        what is going on within a network/workstation.

    Routine log analysis benefits network security, 
        as it is beneficial for identifying security incidents, policy violations, 
        fraudulent activity, and operational problems. Logs are also useful 
        when performing auditing and forensic analysis, supporting internal 
        investigations, establishing baselines, and identifying operational 
        trends and long-term problems. Log studies are valuable as they 
        represent traces of naturalistic human behavior uninfluenced by observers. 
        Because log data portrays uncensored behavior, they give a more complete, 
        accurate picture of all behaviors. Logs also have the benefit of being easy 
        to capture at scale. Log studies can easily include data from tens 
        or hundreds of millions of people.


    Log Collections
        Deeper examination of log collection sources and the benefits they have to network security.

        Security Software: This is a major source of computer security log data. 
            It includes antimalware software, which records instances of detected malware, 
            when malware scans are performed, and when antivirus signatures are updated.
        
        IDS/IPS: This records detailed information on suspicious behavior, detected attacks, 
            and any actions the intrusion prevention systems performed to stop malicious activity 
            in progress. It includes file integrity checks, which can run periodically or continuously.
        
        Remote Access Software: This is often granted from a VPN and logs successful/failed 
            attempts with date/time stamps.
        
        Web Proxies: These are intermediate hosts through which websites are accessed. 
            Proxies make page requests on behalf of users and cache copies of web pages. 
            It can also be used to restrict web access and add a layer of protection between 
            web clients and web servers. They often keep records of all URLs accessed through them.

        Vulnerability Management Software: This logs the patch installation history and vulnerability 
            status of each host, which includes known vulnerabilities, missing software updates, 
            logs host configuration, which it runs occasionally rather than continuously, 
            and authentication servers (log authentication attempts, to include origin, 
            username, success/failures and timestamps).
        
        Routers: These may be configured to permit or block certain types of network traffic.
        
        Firewalls: These block activity based on policy, track the state of network traffic, 
            and usually generate more detailed logs than routers.
        
        Network Quarantine Servers: Some organizations check each remote host’s security posture 
            before allowing it to join the network. Hosts that do not respond to the check or 
            fail the check are quarantined on a separate virtual local area network (VLAN); 
            log information about the status of checks, including which hosts were quarantined and for what reason.
        
        Operating Systems (OS): OS logs are most beneficial for identifying or investigating suspicious activity 
            involving a particular host. After the security software identifies the suspicious activity, 
            OS logs are often consulted to get more information. For example, a network security device 
            might detect an attack against a particular host; that host’s OS logs might indicate if a user 
            was logged into the host at the time of the attack and if the attack was successful. 
            System events are operational actions performed by OS components, such as shutting down the system or 
            starting a service. The details logged for each event also vary widely; each event is usually timestamped, 
            and other supporting information could include event, status, and error codes; service name; 
            and the user or system account associated with an event. Audit records contain security event information 
            such as successful and failed authentication attempts, file accesses, security policy changes, 
            account changes (e.g., account creation and deletion, account privilege assignment), 
            and use of privileges. OS’ typically permit system administrators to specify which types of 
            events should be audited and whether successful or failed attempts to perform certain actions should be logged.
        
        Applications: Some applications generate their own log files, while others use the logging capabilities 
            of the OS on which they are installed. Applications vary significantly in the types of information that they log.


Log Parsing and Log Types
    Log parsing is splitting data into chunks of information that are easier to manipulate and store. 
        Logs contain multiple pieces of information stored as text, and the goal of parsing is to recognize 
        and group them in a meaningful way that analysts can interpret. Since organizations can utilize many 
        different log sources, interpreting data can sometimes be overwhelming.

    Aggregation and Normalization
        We will now take a deeper dive into what is happening behind the scenes after a malicious link is clicked! 
        Let us imagine we have all turned into a series of 1’s and 0’s, i.e.,  computer language. 
        Let us observe what is happening behind the screens of things we may not necessarily normally observe, 
        but are happening to our computers.

        Logs often provide data in a single string, and parsing converts the single string of data into files 
            of structured data. If we separate the values, we can categorize each field and rearrange them 
            to match a uniform structure, a process known as log normalization.

        Log aggregation is the identification and collection of logs from multiple computing sources. 
            Logs from different sources are often created in different formats, even if they are 
            logging similar data. This means that we need to parse and normalize the logs.

        Log normalization is standardizing fields in data from different sources and formats so they 
            can be analyzed together. It helps identify the common attributes between the two logs 
            and define them into fields to be analyzed together. Common fields that can be identified between 
            logs, such as USER, SOURCE IP, and DESTINATION IP, help cyber professionals to have a general 
            understanding of the data in the logs to parse them accurately and determine each field.

        43.182.12.35| New Client Connection |84.10.8.22| on account:| PSmith| :Success
        #Source IP                   #Destination IP                  #User

        User |TJones| Successfully Authenticated | to | 10.182.12.35 |from client |43.10.8.22|
                                                  #Destination IP            #Source IP
    
    Log Types

        There are additional log types monitored by SIEM (Security Information and Event Management) devices. 
        These include perimeter device logs, windows event logs, endpoint logs, application logs, proxy logs, 
            and Internet of Things (IoT) logs.

            2015-07-06 11:35:26 ALLOW TCP 10.40.4.182 10.40.1.11 63064 135 0 - 0 0 0 - - - SEND

        Perimeter devices monitor and regulate traffic to and from the network. 
            Examples of Perimeter device logs are firewalls, VPNs, intrusion detection systems (IDSs), 
            and intrusion prevention systems (IPSs). These devices generate logs containing a large amount of data, 
            and perimeter device logs are vital for understanding the security events occurring in the network. 
            Perimeter devices allow security analysts to detect malicious traffic on a network, detect security 
            misconfigurations, and detect attacks.

        As you can observe from this example log, first, a timestamp is specified. 
            This is followed by an action. In this case, the action is allowing a TCP connection. 
            We can also see the action commanded IP address and ports used from the source and destination IP. 
            With this type of log data, security analysts can detect attempts made to connect to ports. 
            If the ports are not usually used, this can indicate malicious traffic.
        
            Warning 4/28/2020 12:32:47 PM WLAN-AutoConfig 4003 None
        
        Windows event logs are records of essentially everything that happens on a Windows operating system. 
            Most critical servers run on the Windows platform. Therefore, it is essential to monitor this
            log data to understand what is happening to your critical resources. Additionally, Windows logs 
            provide insight into how a workstation is functioning. If a workstation were to be attacked, 
            these logs could help reconstruct user activities to give a timeline analysis.

        In the above example, first, the classification of the event is shown. 
            Classifications are events based on severity as Warning, Information, Critical, and Error. 
            This example is from the WLAN AutoConfig service, a connection management utility enabling users 
            to connect to a wireless local area network (WLAN) dynamically. We next observe the timestamp of the event. 
            We next find the error code, signaling that the WLAN has limited connectivity with it disconnecting. 
            With that in mind, an analyst could troubleshoot for additional network connectivity issues.
        
            Error 6/20/2019 5:00:45 PM Terminal Services- Printers 1111 None

        Endpoint logs are logs from devices across a network, such as desktops, laptops, smartphones, and printers. 
            Monitoring endpoints allow analysts to monitor activities on removable disks, as they are usually 
            more vulnerable to malware installations and data exfiltration attempts, as well as monitor user activity.

        The above log shows an error with the Terminal Services Print driver. 
            By researching Event ID 1111, we know there is a driver issue with the printer.

            02-AUG-2013 17:38:48 * (CONNECT_DATA=(SERVICE_NAME=dev12c)
            (CID=(PROGRAM=sqlplus)(HOST=oralinux1)(USER=oracle))) *
            (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.2.121)(PORT=21165))
            * establish * dev12c * 0

        Application logs are vital to businesses as they typically run on various applications such as databases, 
            web servers, and other in-house applications. 
            Analysts can troubleshoot issues and monitor activities with this type of log data.

        In the above log example, we can observe a connection attempt from a host. 
            We can also observe the date and time of the request and valuable user and host 
            information from which the request originated, in addition to the IP address and the port number.

            4/8/2020 2:20:55 PM User-001 192.168.10.10 GET https://wikipedia.com/

        Proxy servers provide privacy, a means of regulating access, and saving bandwidth on a network. 
            Since web requests, as well as responses, pass through proxy servers, the proxy logs allow 
            analysts to find valuable information about users' usage statistics and browsing behaviors. 
            This can be used to establish baseline behaviors as well.

        The example log above shows User-001 requesting to browse Wikipedia. 
            We are also able to observe the timestamp of the request. 
            These types of logs can also help establish patterns in case of a cyber event on the network.

        Internet of Things (IoT) refers to a network of physical devices exchanging data with other Internet devices. 



Log Parsing
    Logs are used to record information about software systems, operating systems, timestamps of events, 
        and the state of task execution, among other things. 
        With the ever-increasing scale and complexity of systems/environments, the volume of logs is rapidly growing. 
        The traditional ways of log analysis have changed from manually assessing each individual log to 
        more automated procedures. As log records are typically unstructured, to capture these logs automatically, 
        they must be parsed, whereby unstructured ‘raw’ log messages can be transformed into structured, human-readable events.

    One method of parsing logs is using a SIEM, 
        or Security Information and Event Management tool, such as Splunk. 
        With Splunk, analysts can search, monitor, and analyze machine-generated data. 
       


MITRE

    MITRE ATT&CK is a knowledge base of adversary tactics and techniques based on real-world observations. 
        The ATT&CK knowledge base is used to develop specific threat models and methodologies in the private sector, 
        in government, and the cybersecurity product and service community.

    The MITRE ATT&CK framework helps to give a common language for offensive and defensive cyber security teams 
        to use when attempting to identify common tactics, techniques, and protocols that advanced persistent 
        threats use against enterprise networks. Information in the framework comes from publically available 
        threat intelligence and incident response reporting and is updated bi-annually.

    Let us think back to the Cyber Kill Chain. The MITRE ATT&CK framework and the Cyber Kill Chain complement each other. 
        The Cyber Kill Chain uses ordered phases to describe high-level adversary objectives. 
        At the same time, ATT&CK Tactics are unordered and may not all occur in a single intrusion because 
        adversary tactical goals change throughout an operation.

    The MITRE ATT&CK framework provides a knowledge base of threat models and practices by investigating 
        real-world scenarios of different adversarial behavior. It provides an adversary tactic and technique taxonomy. 
        The framework contains a comprehensive matrix of tactics and techniques for multiple platforms. 
        Some tactics are Defense Evasion, Discovery, Persistence, Privilege Escalation, Reconnaissance, etc. 
        Each tactic contains a list of techniques that refer to the method or type of attack. 
        For instance, the Privilege Escalation tactic includes techniques such as Access Token Manipulation, 
        Hijack Execution Flow, Process Injection, and the like. Each technique contains information on how to 
        deploy the attack, such as required platforms, required permissions, defenses bypassed, etc., 
        and how to detect attacks from processes and mitigate the attacks with different strategies.


Endpoint Attack Tactics
    Log analysis is defined as taking computer-generated records and being able to decipher them into human-readable content. 
        The assessment of these records is log analysis. Through log analysis, organizations can identify potential 
        threats and root causes to respond and mitigate tasks properly.

    Specialized tools are used to collect information from various sources to give insight asandctionable data 
        that IT professionals can use to perform log analysis and mitigation. One such tool we will examine in the next lesson is Splunk. 
        We began to get familiar with this tool in the last lesson, but we will be working within the tool for the next coming lessons.

    Remote and mobile work has been on a steady rise in the past few years. While remote work has many benefits, 
        it has also turned endpoints, such as laptops, smartphones, and IoT devices, into a major security concern. 
        Criminals use tactics such as ransomware and malware at conduct endpoint attacks. 
        Additionally, cybercriminals have begun using file-less attacks to target endpoints that traditional 
        antivirus technologies cannot detect. As the cyber “landscape” evolves, traditional endpoint 
        technologies cannot keep up. These tools are often misconfigured or simply unable to detect 
        these ever-evolving threats. Without an endpoint solution plan, it is getting harder to protect data.

    There are several hunting points to aid cyber security professionals when looking into different endpoint attacks:
        Indicators of Compromise (IOCs): These are factors, including forensics data and log files, 
            that can help identify potentially malicious activity that has already occurred. 
        
        Indicators of Attack (IOAs): These are similar to IOCs; however, rather than focusing on the analysis 
            of a compromise that has already taken place, Indicators of Attack focus on i
            dentifying attacker activity while an attack is in process.

        Network-Based Artifacts: This is searching for malware communication using tools such as session recording, 
            packet capture, and network state monitoring. Since most malware communicates with external entities 
            through the network, threat hunters can use artifacts such as listening ports (TCP/UDP) 
            as they could contain malicious content.

        Host-Based Artifacts: This is searching endpoints and looking for malware interaction within the registry, 
        file system, and elsewhere. Threat hunters typically look through the registry since most tools 
        and malware interact and store configuration information in it. Analyzing the file system allows 
        threat hunters to view activities such as suspicious reading, writing, deletion, and the destruction of information.

    Defense in Depth
        Oftentimes, the actions that attackers take are the same actions taken legitimately by users millions 
            upon millions of times a day. From authenticating to a domain controller to mounting a network share, 
            these are legitimate actions that can be seen every minute of every day on a normal network. 
            These can also be signs of malicious activity.

        Things the attacker needs:
            Execution
            Credentials
            Enumeration
            Authentication
            Data Movement

        During an active intrusion, attackers are very goal focused. 
            Their actions typically can be associated with one of the above needs, and you may see multiple needs 
            from a single entity in very short periods. If you contrast that to how a normal user appears on a network, 
            the behavior is typically very different. We can use these differences to our advantage. Instead of searching 
            for actions that would likely produce millions of results, we can search for patterns of behaviors 
            that fall into multiple needs.

        A defense-in-depth strategy is an approach that uses multiple layers of security for protection in the event 
            an attacker breaches one layer. Layered defense mechanisms allow for the reduction of vulnerabilities, 
            help to contain threats, and help to mitigate risks. This concept originates from the National Security Agency 
            (NSA) and gets its name through common military approaches. The overall goal is to stop threats/breaches before 
            they happen. Still, a defense-in-depth strategy allows for the discovery and mitigation of threats already underway, 
            preventing additional damages from taking place. There are several key areas of control that a defense-in-depth 
            tiered structure allow:
                Physical controls: badges, key cards, finger scanner, etc.
                Network security controls: authentication software, etc.
                Administrative controls: MFA, only allowing access to what employees need, etc.
                Antivirus: slowing the spread of malicious software
                Behavioral analysis: Incorporating baseline analysis to determine common and uncommon network behaviors

        Defense-in-depth strategies were built around perimeter-based security models in the past. 
            However, a traditional perimeter-based model is no longer as effective in the growing digital world. 
            Many organizations now have applications in data centers, cloud infrastructure (both private and public), 
            and even Software as a Service products, such as Google Workspace or MS365. Attackers can breach 
            those applications and fly under the radar for weeks, making new defense-in-depth strategies a must 
            to mitigate the threats. Some modern defense-in-depth strategies can include:
                Protecting privileged access
                Lockdown endpoints
                Multi-factor authentication
            
        Security is fascinating because some of the first things we learn can come back and 
            be reintroduced as we gain knowledge and understanding of the threat hunting/mitigation process.

Vulnerability Scanning
    Vulnerability scanning is the inspection of the potential points of exploit on a computer or network to identify 
        security holes. There are two types of vulnerability scanning: internal and external. 
        You can think of it like a house. External vulnerability scanning is equivalent to checking if doors and 
        windows are locked. Internal vulnerability scanning is checking if bedroom doors are locked. 
        Oftentimes, compromises lead to extensive damage to organizations. With vulnerability scanning, 
        compromises could potentially have been avoided by testing the environments. Vulnerabilities must be 
        continuously identified, prioritized, and remediated to reduce risks and prevent attacks/breaches. 
        According to Norton, there are around 2,200 cyberattacks per day, and without regular vulnerability scanning, t
        he probability of being exploited and compromised increases considerably.

    Once scanning has been conducted, an extensive report is typically generated outlining the found vulnerabilities. 
        It is important to act quickly on the discovered vulnerabilities to ensure that security holes are fixed. 
        It is also important to re-run the vulnerability scans to validate that the flaws have been addressed. 

    Vulnerability scanning is often confused with penetration testing. The key difference between the two is that 
        vulnerability scans are automated, while penetration testing involves a live person probing a network and 
        actively attempting to exploit vulnerabilities.